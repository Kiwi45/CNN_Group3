{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Introduction - Webscraping\n",
    "\n",
    "\n",
    "## Contents \n",
    "\n",
    "[1. Rules](#Rules)\n",
    "\n",
    "[2. APIs](#APIs)\n",
    "\n",
    "[3. Websites](#Websites)\n",
    "\n",
    "[4. Exercises](#Exercises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules\n",
    "\n",
    "1. Respect the wishes of the targeted website(s):\n",
    "\n",
    "$\\qquad$ - Checks if an API is available or if the data can be downloaded otherwise <br>\n",
    "$\\qquad$ - Keep in mind where the data comes from, respect copyrights and refer to the source if necessary.<br>\n",
    "$\\qquad$ - Play with open cards, i.e. don't identify yourself as a normal internet user<br>\n",
    "\n",
    "2. Waits one or two seconds after each request\n",
    "\n",
    "$\\qquad$ - Scrape only what is needed for your project and only once (e.g. save html data on your hard disk and edit it afterwards)\n",
    "\n",
    "3. How do we find out if the access is authorized?\n",
    "\n",
    "$\\qquad$ - Some websites prohibit the access of scrapers via a robots.txt file.<br>\n",
    "$\\qquad$ - Also in the Terms of Service (AGB's) you will often find hints if scraping is allowed. In case of doubt, always contact the operators first.\n",
    "\n",
    "### Example ResearchGate.net - is webscraping allowed?\n",
    "\n",
    "Check https://www.researchgate.net/robots.txt:\n",
    "\n",
    "````\n",
    "User-agent: *\n",
    "Allow: /\n",
    "Disallow: /connector/\n",
    "Disallow: /deref/\n",
    "Disallow: /plugins.\n",
    "Disallow: /firststeps.\n",
    "Disallow: /publicliterature.PublicLiterature.search.html\n",
    "Disallow: /amp/authorize\n",
    "Allow: /signup.SignUp.html\n",
    "Disallow: /signup.\n",
    "````\n",
    "\n",
    "User-agent: * means, that the following conditions apply to all User agent types (e.g. Google Bots or our Python application).\n",
    "Within the rest of the file, it is defined which parts of the website are prohibited to scrape: e.g. `/connector/`.\n",
    "\n",
    "Even though it seems like this website allows us to scrape their content, the terms and conditions might indicate something different:\n",
    "\n",
    "Within the [Terms of Service](https://www.researchgate.net/application.TermsAndConditions.html) it is clearly stated, that the website provider does not allow webscraping:\n",
    "<img src=\"https://www.dropbox.com/s/6o3m0yj59j9ks9t/researchgate_tos.PNG?dl=1\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "\n",
    "**Conclusion**: You should not scrape the site without the permission of the operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "\n",
    "Lots of websites offer free APIs in order to access their data. Please note, that using the API instead of scraping the website directly is considered best practice if applicable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Powerful package introducing the datastructure DataFrame\n",
    "\n",
    "crix = pd.read_json(\"http://data.thecrix.de/data/crix.json\")\n",
    "crix.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note, that live isn't always that easy and usually you have a lot more to do in order to download your desired information. Sometimes the information you are interested in is spread over severeal JSON files respectively links. In this case, one needs to loop over all relevant links in order to retrieve the necessary information. Please make sure, that you are not overloading the server, since they might block you in case of to many requests within a certain amount of time. A good rule of thumb is to not send more than 1-2 requests per second if nothing is stated by the webiste operator. You can do so using the time package and the sleep function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# example to illustrate the sleep function -> print the counter i and then do nothing for 2 seconds\n",
    "for i in range(0,5):\n",
    "    print(i+1)\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try an API that contains more data and is therefore more complex in terms of data extraction. To do so, we will use the coingecko API on cryptocurrency data. The API documentation and Terms of use can be found [here](https://www.coingecko.com/en/api#explore-api)\n",
    "\n",
    "\n",
    "The APIs base link is as follows: https://api.coingecko.com/api/v3/\n",
    "\n",
    "So lets try it and lets find out the number of cryptocurrencies available via this API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save base link in variable\n",
    "base = 'https://api.coingecko.com/api/v3/'\n",
    "data = pd.read_json(base + 'coins/list')\n",
    "print(data.shape) # -> 4454 cryptos are currently available via coingecko\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we now how to extract the symbols, we want to extract certain data. We are for example interested in the price and market capitalization in usd for bitcoin and ethereum. We therefore run the follwoing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the ids of interest\n",
    "ids = [data[data['name']==elem]['id'].values.tolist()[0] for elem in ['Bitcoin','Ethereum']]\n",
    "\n",
    "# extract the relevant information\n",
    "pd.read_json(base + 'coins/' + ids[0]) # -> due to json structure we cannot directly access the file using pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the different lengths in the json file, we need to use a different packages to load the data in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import urllib.request as request\n",
    "\n",
    "df = pd.DataFrame(columns=['id','name','symbol','price','market_cap'])\n",
    "\n",
    "for idx in ids:\n",
    "    with request.urlopen(base + 'coins/' + idx) as response:\n",
    "            source = response.read()\n",
    "            data = json.loads(source)\n",
    "        \n",
    "    line_as_dict = {'id': data['id'],\n",
    "                    'name': data['name'],\n",
    "                    'symbol': data['symbol'],\n",
    "                    'price': data['market_data']['current_price']['usd'],\n",
    "                    'market_cap': data['market_data']['market_cap']['usd']}\n",
    "    df = df.append(line_as_dict, ignore_index = True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Websites\n",
    "\n",
    "In case of no available API, one needs to scrape the information of interest directly of the html code from the website itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML -  Hypertext Markup Language\n",
    "\n",
    "[HTML](https://en.wikipedia.org/wiki/HTML) is a prgramming language to structure digital documents and consists of multiple elements which are organized in a tree structure. Elements are usually buildt using three different structures:\n",
    "\n",
    "```html\n",
    "<a href=\"https://www.hu-berlin.de/\">Link to HU Berlin</a>\n",
    "```\n",
    "\n",
    "1. Leading and closing  **Tags**.\n",
    "2. **Attributes** are set within the tags\n",
    "3. The **Text** that needs to be structured\n",
    "\n",
    "What we see in the browser is the interpretation of the HTML document:\n",
    "\n",
    "\n",
    "````\n",
    "Elements: <head>, <body>, <footer>...\n",
    "Components: <title>,<h1>, <div>...\n",
    "Text Styles: <b>, <i>, <strong>...\n",
    "Hyperlinks: <a>\n",
    "````\n",
    "\n",
    "Next to HTML CSS and Javascript are also relevant for webscraping:\n",
    "\n",
    "#### CSS\n",
    "- [Cascading Style Sheets](https://en.wikipedia.org/wiki/Cascading_Style_Sheets) (CSS) describe the format and for example colouring of HTML components (e.g. ``<h1>``, ``<div>``...)\n",
    "- CSS is useful for us, due to the fact that the CSS pointer (selector) might be used to find HTML elements.\n",
    "\n",
    "#### Javascript\n",
    "- [Javascript](https://en.wikipedia.org/wiki/JavaScript) extends the functionality of websites (e.g. hide and display certain objects based on user input)\n",
    "\n",
    "### HTML in Chrome Browser\n",
    "\n",
    "- Open [HU Berlin Website](https://www.hu-berlin.de) in Chrome and open Chromes Developer Tools\n",
    "    <img src=\" https://developer.chrome.com/devtools/images/elements-panel.png\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "    \n",
    "- Hover over different elements within the Developer Console and check what will be dosplayed on the regular website.\n",
    "- In the Developer Console one can see all relevant information in regards to certain HTML objects, e.g. `id`, `class`, etc..\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "BeautifulSoup is a Python Parser Packagewhich reads in HTML and XML strings. The documentation can be found here [here](http://www.crummy.com/software/BeautifulSoup/bs4/doc/#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html_doc = \"\"\"\n",
    "<html><head><title>The Dormouse's story</title></head>\n",
    "<body>\n",
    "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
    "\n",
    "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
    "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
    "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
    "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
    "and they lived at the bottom of a well.</p>\n",
    "\n",
    "<p class=\"story\">...</p>\n",
    "\"\"\"\n",
    "\n",
    "# requests package lets you request the HTML code of a certain website \n",
    "# -> requests.get(\"type in your URL of interest here\").text \n",
    "\n",
    "soup = BeautifulSoup(html_doc, \"html5lib\") \n",
    "# html5lib ->  Parser, ggf. vorher über pip installieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently one can retrieve certain attributes from the tree structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially, searching the complete HTML document using certain tags helps us to find the information we are interested in (e.g. find all links on a webpage):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, elements might be selected using its id, href, or class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('id',  soup.find(id=\"link2\"))\n",
    "print('----')\n",
    "print('href', soup.find(href='http://example.com/lacie'))\n",
    "print('----')\n",
    "print('class', soup.find(class_='story'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also search the document by applying regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# again find all links\n",
    "soup.find_all('a', id=re.compile('link[\\d]+'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary -> The Web Scraping - How To\n",
    "\n",
    "1.  Intensively check the webiste structure\n",
    "2.  Choose your scraping strategy\n",
    "3.  Write your Prototype: Extract, process and validate data\n",
    "4.  Generalize: Functions, Loops, Debugging\n",
    "5.  Data preparation (Store, clean and make accessible)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### 1. VCRIX API\n",
    "\n",
    "Write a function that triggers once a day automatically and downloads the HF Crix (http://data.thecrix.de/data/crix_hf.json). The returned results shall be written into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions will be discussed in class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. HU Berlin\n",
    "\n",
    "Write a funtion that saves all external links on the main page of [HU Berlin](https://www.hu-berlin.de/de) and the corresponding timestamp of retrieval into a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions will be discussed in class"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
